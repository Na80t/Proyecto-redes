{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploración Profunda de Emociones Textuales\n",
        "\n",
        "Hoy en día las redes sociales son una plataforma poderosa para la autoexpresíon y, al mismo tiempo, un lugar donde se puede detectar el estado emocional de las personas. Buscamos no solo identificar la naturaleza emocional de los textos sino también proporcionar una herramienta analítica que pueda ser  útil para intervenir de manera oportuna en situaciones de crisis emocional o psicológica. Esto podría ser de gran ayuda para organizaciones dedicadas al bienestar mental, empresas de redes sociales, y agencias gubernamentales interesadas en monitorear y mejorar la salud emocional de la población.\n",
        "\n",
        "Tendremos un como entrada un conjunto de datos obtenidos de la plataforma Kaggle. Este dataset cuenta con 1.6 millones de tweets, donde estos se encuntran claseificados en 0 = negativo, 2 = neutral y 4 = positivo. También podemos encontrar ids, date, flag, user y text. Con esto podremos clasificar nustro texto en positivo y negativo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ADQUISICIÓN DE LOS DATOS**\n",
        "\n",
        "El conjunto de datos a utilizar es \"Sentiment140 dataset with 1.6 million tweets\" obtenido de https://www.kaggle.com/datasets/kazanova/sentiment140/data. Una vez que se descargue, se debe cambiar el nombre del archivo a 'train.csv'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ne9Op0EMsWO0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExrijI8nR15V"
      },
      "source": [
        "\n",
        "\n",
        "**ADQUISICIÓN DE LOS DATOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is5Zfem2zcYs",
        "outputId": "1e824134-3c97-42af-b220-8e6dbe745408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    target         ids                          date      flag  \\\n",
            "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
            "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
            "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
            "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "5        0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
            "6        0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
            "7        0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
            "8        0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
            "9        0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
            "10       0  1467812416  Mon Apr 06 22:20:16 PDT 2009  NO_QUERY   \n",
            "11       0  1467812579  Mon Apr 06 22:20:17 PDT 2009  NO_QUERY   \n",
            "12       0  1467812723  Mon Apr 06 22:20:19 PDT 2009  NO_QUERY   \n",
            "13       0  1467812771  Mon Apr 06 22:20:19 PDT 2009  NO_QUERY   \n",
            "14       0  1467812784  Mon Apr 06 22:20:20 PDT 2009  NO_QUERY   \n",
            "15       0  1467812799  Mon Apr 06 22:20:20 PDT 2009  NO_QUERY   \n",
            "16       0  1467812964  Mon Apr 06 22:20:22 PDT 2009  NO_QUERY   \n",
            "17       0  1467813137  Mon Apr 06 22:20:25 PDT 2009  NO_QUERY   \n",
            "18       0  1467813579  Mon Apr 06 22:20:31 PDT 2009  NO_QUERY   \n",
            "19       0  1467813782  Mon Apr 06 22:20:34 PDT 2009  NO_QUERY   \n",
            "\n",
            "               user                                               text  \n",
            "0   _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
            "1     scotthamilton  is upset that he can't update his Facebook by ...  \n",
            "2          mattycus  @Kenichan I dived many times for the ball. Man...  \n",
            "3           ElleCTF    my whole body feels itchy and like its on fire   \n",
            "4            Karoli  @nationwideclass no, it's not behaving at all....  \n",
            "5          joy_wolf                      @Kwesidei not the whole crew   \n",
            "6           mybirch                                        Need a hug   \n",
            "7              coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
            "8   2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
            "9           mimismo                          @twittera que me muera ?   \n",
            "10   erinx3leannexo        spring break in plain city... it's snowing   \n",
            "11     pardonlauren                         I just re-pierced my ears   \n",
            "12             TLeC  @caregiving I couldn't bear to watch it.  And ...  \n",
            "13  robrobbierobert  @octolinz16 It it counts, idk why I did either...  \n",
            "14      bayofwolves  @smarrison i would've been the first, but i di...  \n",
            "15       HairByJess  @iamjazzyfizzle I wish I got to watch it with ...  \n",
            "16   lovesongwriter  Hollis' death scene will hurt me severely to w...  \n",
            "17         armotley                               about to file taxes   \n",
            "18       starkissed  @LettyA ahh ive always wanted to see rent  lov...  \n",
            "19        gi_gi_bee  @FakerPattyPattz Oh dear. Were you drinking ou...   \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Descargar el dataset desde https://www.kaggle.com/datasets/kazanova/sentiment140/data\n",
        "#Una vez que se descargue, se debe cambiar el nombre del archivo a train.csv\n",
        "\n",
        "#Cargamos el dataset\n",
        "dataset = pd.read_csv('train.csv', encoding='ISO-8859-1', header=None, names=['target', 'ids', 'date', 'flag', 'user', 'text'])\n",
        "\n",
        "#Mostramos las primeras filas del dataset\n",
        "print(dataset.head(20), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62Zwi6te5KWj"
      },
      "source": [
        "**PROCESAMIENTO DE LOS DATOS Y EXTRACCIÓN DE LAS CARACTERISTICAS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['target', 'ids', 'date', 'flag', 'user', 'text'], dtype='object')"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.columns #Mostramos las columnas del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "user\n",
              "lost_dog           549\n",
              "webwoke            345\n",
              "tweetpet           310\n",
              "SallytheShizzle    281\n",
              "VioletsCRUK        279\n",
              "mcraddictal        276\n",
              "tsarnick           248\n",
              "what_bugs_u        246\n",
              "Karen230683        238\n",
              "DarkPiano          236\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['user'].value_counts().head(10) #Mostramos los usuarios que más tweets han hecho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "pDTmz9-9cnJK"
      },
      "outputs": [],
      "source": [
        "# Función para limpiar el texto\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = re.sub(r'@[A-Za-z0-9]+', '', texto)  # Eliminar menciones\n",
        "    texto = re.sub(r'https?://[A-Za-z0-9./]+', '', texto)  # Eliminar enlaces\n",
        "    texto = re.sub(r'#', '', texto)  # Eliminar hashtags\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Eliminar signos de puntuación\n",
        "    texto = texto.lower()  # Convertir a minúsculas\n",
        "    return texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0        awww thats a bummer  you shoulda got david ...\n",
            "1     is upset that he cant update his facebook by t...\n",
            "2      i dived many times for the ball managed to sa...\n",
            "3       my whole body feels itchy and like its on fire \n",
            "4      no its not behaving at all im mad why am i he...\n",
            "5                                   not the whole crew \n",
            "6                                           need a hug \n",
            "7      hey  long time no see yes rains a bit only a ...\n",
            "8                           _k nope they didnt have it \n",
            "9                                        que me muera  \n",
            "10              spring break in plain city its snowing \n",
            "11                            i just repierced my ears \n",
            "12     i couldnt bear to watch it  and i thought the...\n",
            "13     it it counts idk why i did either you never t...\n",
            "14     i wouldve been the first but i didnt have a g...\n",
            "15     i wish i got to watch it with you i miss you ...\n",
            "16    hollis death scene will hurt me severely to wa...\n",
            "17                                 about to file taxes \n",
            "18     ahh ive always wanted to see rent  love the s...\n",
            "19     oh dear were you drinking out of the forgotte...\n",
            "Name: text_clean, dtype: object \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Aplicar la función de limpieza al texto\n",
        "dataset['text_clean'] = dataset['text'].apply(limpiar_texto)\n",
        "\n",
        "# Eliminar columnas innecesarias\n",
        "dataset = dataset[['target', 'text_clean']]\n",
        "\n",
        "# Convertir 4 (positivo) a 1 y 0 (negativo) a 0\n",
        "dataset['target'] = dataset['target'].apply(lambda x: 1 if x == 4 else 0)\n",
        "\n",
        "# Mostrar las primeras filas del dataset limpio\n",
        "print(dataset['text_clean'].head(20), '\\n')\n",
        "\n",
        "#eliminar columna de target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instalar: pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Tomar una muestra del dataset para acelerar el entrenamiento\n",
        "sample_size = 10000  # Ajustar el tamaño de la muestra según sea necesario\n",
        "dataset_sample = dataset.sample(n=sample_size, random_state=42)\n",
        "\n",
        "# Separar los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset_sample['text_clean'], dataset_sample['target'], test_size=0.3, random_state=50)\n",
        "\n",
        "# Tokeniza los textos\n",
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "max_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Construye el modelo de red convolucional\n",
        "model = tf.keras.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Dropout(0.5),\n",
        "    BatchNormalization(),\n",
        "    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Dropout(0.5),\n",
        "    BatchNormalization(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
